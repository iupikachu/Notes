## 联邦学习

终端设备的数量及其产生的数据都在迅速增加，如果能利用这些数据来训练机器学习(尤其是近年来取得巨大成功的深度学习)模型，服务提供商可以给用户更好的体验。一个例子是，Google利用用户的打字习惯来训练模型，可以在用户输入上文之后给出最有可能的下文，从而提高用户输入速度。与单机的机器学习不同，该场景下的机器学习是分布式的，具有以下特点

- 需要保护用户隐私。终端设备的数据可能是高度个性化的，不是所有用户都希望这些数据被上传到数据中心，因此模型的训练应该在不传输原始训练数据的前提下进行。
- 数据分布非独立同分布。由于各个用户的使用习惯等因素的差别，不同终端设备收集到的数据很可能不符合相同的分布，从而一个终端设备的数据无法代表全局的数据。
- 数据不平衡。有的用户是手机的重度使用者，而其他用户则不是，因此不同终端设备收集到的数据量可能会有很大差别。不平衡的数据会影响模型训练结果的好坏。
- 通信受限。终端设备的网络连接是不稳定的，并且通信成本高、速度慢等特点进一步限制了数据的传输。很少有人愿意用流量来传输十几个GB的数据。

单机的机器学习无法解决这些问题。一种可行的方法是，每个拥有数据的终端设备利用自己的数据训练局部模型，在训练过程中不同的设备之间相互通信，所有局部模型借助通信整合到一起形成一个全局模型，该全局模型仿佛是收集了所有数据之后训练得到的模型。这便是**联邦学习**(Federated Learning)的思想。如何将局部模型整合成为全局模型是联邦学习的关键问题，FedAvg算法正是为此而提出的。

fedavg: https://ravex.gitee.io/2020/11/07/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0FedAvg%E7%AE%97%E6%B3%95/

### 1. SGD(tochastic Gradient Descent ) 随机梯度下降



假如我们要优化一个函数 ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29) ，即找到它的最小值, 常用的方法叫做Gradient Descent (GD), 也就是最速下降法. 说起来很简单, 就是每次沿着当前位置的导数方向走一小步, 走啊走啊就能够走到一个好地方了.



给GD 加随机噪声随机抽取标本。



在机器学习算法中,有时候需要对原始的模型构建损失函数,然后通过优化算法对损失函数进行优化，以便寻找到最优的参数，使得损失函数的值最小。而在求解机器学习参数的优化算法中，使用较多的就是基于梯度下降的优化算法(Gradient Descent, GD)。

优点：效率。在梯度下降法的求解过程中，只需求解损失函数的一阶导数，计算的代价比较小，可以在很多大规模数据集上应用





### 2. 经验风险最小化

**偏差（Biase）**      偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习**算法本身的拟合能力**。

**方差（Variance）** 方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了**数据扰动**所造成的影响。



<img src="/Users/chenqipeng/Library/Application Support/typora-user-images/image-20210616144613963.png" alt="image-20210616144613963" style="zoom:80%;" />



假设红色的靶心区域是学习算法完美的正确预测值，蓝色点为训练数据集所训练出的模型对样本的预测值，当我们从靶心逐渐往外移动时，预测效果逐渐变差。

从上面的图片中很容易可以看到，左边一列的蓝色点比较集中，右边一列的蓝色点比较分散，它们描述的是方差的两种情况。比较集中的属于方差比较小，比较分散的属于方差比较大的情况。

我们再从蓝色点与红色靶心区域的位置关系来看，靠近红色靶心的属于偏差较小的情况，远离靶心的属于偏差较大的情况。



###  3. FedAvg （联邦平均）

FedAvg算法将多个使用SGD的深度学习模型整合成一个全局模型

